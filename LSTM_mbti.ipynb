{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dCMjp0omHDD4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "k8z5J3voHDD6",
        "outputId": "df32865a-e835-4825-f3ad-f8cdf0ef9c62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-53e2cfee5d48>:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  text=pd.read_csv(\"mb_data_2.csv\" ,index_col='type',engine='python', error_bad_lines=False)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-53e2cfee5d48>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mb_data_2.csv\"\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#print(text.iloc[2])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mb_data_2.csv'"
          ]
        }
      ],
      "source": [
        "text=pd.read_csv(\"mb_data_2.csv\" ,index_col='type',engine='python', error_bad_lines=False)\n",
        "print(text.shape)\n",
        "print(text[0:5])\n",
        "#print(text.iloc[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4ozopegHDD8"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# One hot encode labels\n",
        "labels=text.index.tolist()\n",
        "print(labels[50:55])\n",
        "encoder=LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
        "labels=encoder.fit_transform(labels)\n",
        "labels=np.array(labels)\n",
        "print(labels[50:55])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WdhlSx7iHDD9"
      },
      "outputs": [],
      "source": [
        "mbti_dict={0:'ENFJ',1:'ENFP',2:'ENTJ',3:'ENTP',4:'ESFJ',5:'ESFP',6:'ESTJ',7:'ESTP',8:'INFJ',9:'INFP',10:'INTJ',11:'INTP',12:'ISFJ',13:'ISFP',14:'ISFP',15:'ISTP'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "oRXjfXmcHDD9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Function to clean data..\n",
        "def post_cleaner(post):\n",
        "    \"\"\"cleans individual posts`.\n",
        "    Args:\n",
        "        post-string\n",
        "    Returns:\n",
        "         cleaned up post`.\n",
        "    \"\"\"\n",
        "    # Covert all uppercase characters to lower case\n",
        "    post = post.lower() \n",
        "    \n",
        "    # Remove |||\n",
        "    post=post.replace('|||',\"\") \n",
        "\n",
        "    # Remove URLs, links etc\n",
        "    post = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', '', post, flags=re.MULTILINE) \n",
        "    # This would have removed most of the links but probably not all \n",
        "\n",
        "    # Remove puntuations \n",
        "    puncs1=['@','#','$','%','^','&','*','(',')','-','_','+','=','{','}','[',']','|','\\\\','\"',\"'\",';',':','<','>','/']\n",
        "    for punc in puncs1:\n",
        "        post=post.replace(punc,'') \n",
        "\n",
        "    puncs2=[',','.','?','!','\\n']\n",
        "    for punc in puncs2:\n",
        "        post=post.replace(punc,' ') \n",
        "    # Remove extra white spaces\n",
        "    post=re.sub( '\\s+', ' ', post ).strip()\n",
        "    return post"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9HAcZ6r8HDD-"
      },
      "outputs": [],
      "source": [
        "posts=text.posts.tolist()\n",
        "posts=[post_cleaner(post) for post in posts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMpWKf9THDD_"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "word_count=Counter()\n",
        "for post in posts:\n",
        "    word_count.update(post.split(\" \"))\n",
        "    \n",
        "vocab_len=len(word_count)\n",
        "print(vocab_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IG1D_qRUHDD_"
      },
      "outputs": [],
      "source": [
        "vocab = sorted(word_count, key=word_count.get, reverse=True)\n",
        "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
        "posts_ints=[]\n",
        "\n",
        "for post in posts:\n",
        "    posts_ints.append([vocab_to_int[word] for word in post.split()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrSa5B3IHDEA"
      },
      "outputs": [],
      "source": [
        "print(posts_ints[0][:10])\n",
        "print(len(posts_ints[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LYsCks9HDEA"
      },
      "outputs": [],
      "source": [
        "posts_lens = Counter([len(x) for x in posts])\n",
        "print(posts_lens)\n",
        "print(#np.mean(posts_lens.keys()) #(0.5*np.std(posts_lens.keys()))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jwxU8jdlHDEB"
      },
      "outputs": [],
      "source": [
        "seq_len = 1000\n",
        "features=np.zeros((len(posts_ints),seq_len),dtype=int)\n",
        "for i, row in enumerate(posts_ints):\n",
        "    features[i, -len(row):] = np.array(row)[:seq_len]\n",
        "#print(features[1555])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "C9jQw75kHDEB"
      },
      "outputs": [],
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "num_ele=int(split_frac*len(features))\n",
        "rem_ele=len(features)-num_ele\n",
        "train_x, val_x = features[:num_ele],features[num_ele: num_ele + int(rem_ele/2)]\n",
        "train_y, val_y = labels[:num_ele],labels[num_ele:num_ele + int(rem_ele/2)]\n",
        "\n",
        "test_x =features[num_ele + int(rem_ele/2):]\n",
        "test_y = labels[num_ele + int(rem_ele/2):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1C256czNHDEC"
      },
      "outputs": [],
      "source": [
        "lstm_size = 256\n",
        "lstm_layers = 1\n",
        "batch_size = 256\n",
        "learning_rate = 0.01\n",
        "embed_dim=250\n",
        "n_words = len(vocab_to_int) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmDq6FJZZtQM"
      },
      "outputs": [],
      "source": [
        "!pip install opendatasets\n",
        "import opendatasets as od\n",
        "\n",
        "od.download(\"https://www.kaggle.com/datasets/leadbest/googlenewsvectorsnegative300\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UV4PLMsgHDEC"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
        "int_to_vocab = {ii: word for word,ii in vocab_to_int.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5JS5fo2HDEC"
      },
      "outputs": [],
      "source": [
        "#model[int_to_vocab[2]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eX2k4HEHDED"
      },
      "outputs": [],
      "source": [
        "wv_emb = []\n",
        "unk=[]\n",
        "for k in range(1,len(int_to_vocab)):\n",
        "    try:\n",
        "        wv_emb.append(model[int_to_vocab[k]])\n",
        "    except:\n",
        "        unk.append(int_to_vocab[k])\n",
        "len(unk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_vpON9ZHDED"
      },
      "outputs": [],
      "source": [
        "unk[160:170]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rdPUby4jHDED"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    input_data = tf.compat.v1.placeholder(tf.int32, [None, None], name='inputs')\n",
        "    labels_ = tf.compat.v1.placeholder(tf.int32, [None, None], name='labels')\n",
        "    keep_prob = tf.compat.v1.placeholder(tf.float32, name='keep_prob')\n",
        "    \n",
        "    embedding= tf.compat.v1.Variable(tf.random.uniform(shape=(n_words,embed_dim),minval=-1,maxval=1))\n",
        "    embed=tf.compat.v1.nn.embedding_lookup(embedding,input_data)\n",
        "    #print(embed.shape)\n",
        "    \n",
        "    lstm = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
        "    drop = tf.compat.v1.nn.rnn_cell.DropoutWrapper(lstm,output_keep_prob=keep_prob)\n",
        "    cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell([drop]* lstm_layers)\n",
        "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
        "    \n",
        "    outputs,final_state=tf.compat.v1.nn.dynamic_rnn(cell,embed,dtype=tf.float32 )\n",
        "    pre = tf.compat.v1.layers.dense(outputs[:,-1], 16, activation=tf.nn.relu)\n",
        "    predictions=tf.compat.v1.layers.dense(pre, 16, activation=tf.nn.softmax)\n",
        "    \n",
        "    cost = tf.compat.v1.losses.mean_squared_error(labels_, predictions)\n",
        "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "    \n",
        "    correct_pred = tf.compat.v1.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
        "    accuracy = tf.compat.v1.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "G4r0ISRrHDEE"
      },
      "outputs": [],
      "source": [
        "def get_batches(x, y, batch_size=100):\n",
        "    \n",
        "    n_batches = len(x)//batch_size\n",
        "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
        "    for ii in range(0, len(x), batch_size):\n",
        "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQrq8efXHDEE"
      },
      "outputs": [],
      "source": [
        "epochs = 3\n",
        "\n",
        "with graph.as_default():\n",
        "    saver = tf.compat.v1.train.Saver()\n",
        "\n",
        "with tf.compat.v1.Session(graph=graph) as sess:\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "    iteration = 1\n",
        "    for e in range(epochs):\n",
        "        state = sess.run(initial_state)\n",
        "        \n",
        "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
        "            feed = {input_data: x,\n",
        "                    labels_: y,\n",
        "                    keep_prob: 1.0,\n",
        "                    initial_state: state}\n",
        "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
        "            \n",
        "            if iteration%5==0:\n",
        "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
        "                      \"Iteration: {}\".format(iteration),\n",
        "                      \"Train loss: {:.3f}\".format(loss))\n",
        "\n",
        "            if iteration%25==0:\n",
        "                val_acc = []\n",
        "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
        "                for x, y in get_batches(val_x, val_y, batch_size):\n",
        "                    feed = {input_data: x,\n",
        "                            labels_: y,\n",
        "                            keep_prob: 1,\n",
        "                            initial_state: val_state}\n",
        "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
        "                    val_acc.append(batch_acc)\n",
        "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
        "            iteration +=1\n",
        "    saver.save(sess, \"checkpoints/mbti.ckpt\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aScz86Ip53M0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVziTOgbHDEF"
      },
      "outputs": [],
      "source": [
        "#y_test_pr=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "temp=0\n",
        "f1=0\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score\n",
        ")\n",
        "\n",
        "#y_test_predictions=np.array(y_test_pr)\n",
        "test_acc = []\n",
        "with tf.compat.v1.Session(graph=graph) as sess:\n",
        "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
        "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
        "    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
        "        feed = {input_data: x,\n",
        "                labels_: y,\n",
        "                keep_prob: 1,\n",
        "                initial_state: test_state}\n",
        "        #print(\"hey\")\n",
        "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
        "        temp+=sess.run(predictions,feed_dict=feed).shape[0]\n",
        "        y_test_predictions=sess.run(predictions,feed_dict=feed)\n",
        "        #y_test_predictions.flatten()\n",
        "        if y_test_predictions.any():\n",
        "          y_test_predictions = np.hstack(y_test_predictions)\n",
        "        else:\n",
        "          y_test_predictions = y_test_predictions.flatten()\n",
        "        #y_test_predictionss= np.where(y_test_predictions==max(y_test_predictions),1,0)\n",
        "        print(y_test_predictions)\n",
        "        if y.any():\n",
        "          y = np.hstack(y)\n",
        "        else:\n",
        "          y = y.flatten()\n",
        "        for i in range(0,y_test_predictions.shape[0]):\n",
        "          j=0\n",
        "          mx=0\n",
        "          index=0\n",
        "          while j<16 and i+j<4096:\n",
        "            if y_test_predictions[i+j]>mx:\n",
        "              mx=y_test_predictions[i+j]\n",
        "              index=i\n",
        "            j+=1\n",
        "          j=0\n",
        "          while j<16 and i+j<4096:\n",
        "            if y_test_predictions[i+j]==mx:\n",
        "              y_test_predictions[i+j]=1\n",
        "            else:\n",
        "              y_test_predictions[i+j]=0\n",
        "            \n",
        "            j+=1\n",
        "          i=i+15\n",
        "          \n",
        "          \n",
        "        print(y.shape)\n",
        "        print(temp)\n",
        "        f1=f1_score(y_true=y, y_pred=y_test_predictions, average='weighted')\n",
        "        #y_test_predictions=np.concatenate(( y_test_predictions,sess.run(predictions,feed_dict=feed)),axis=0)\n",
        "        test_acc.append(batch_acc)\n",
        "   # print(test_acc)\n",
        "   # print(y_test_predictions)\n",
        "    print(\"Test accuracy: {:.5f}\".format(np.mean(test_acc)))\n",
        "    print(\"F1 scoree: {:.5f}\".format(f1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tempp=text.index.tolist()\n",
        "encoder=LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
        "names_encode = encoder.fit(tempp)\n"
      ],
      "metadata": {
        "id": "Ttx3C8iUupt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_test_pr=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "temp=0\n",
        "f1=0\n",
        "re=0\n",
        "pr=0\n",
        "ac=0\n",
        "Label1pred=[]\n",
        "Label2pred=[]\n",
        "Label3pred=[]\n",
        "Label4pred=[]\n",
        "Label1true=[]\n",
        "Label2true=[]\n",
        "Label3true=[]\n",
        "Label4true=[]\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score\n",
        ")\n",
        "\n",
        "#y_test_predictions=np.array(y_test_pr)\n",
        "test_acc = []\n",
        "f1_total = []\n",
        "recall_total = []\n",
        "prec_total = []\n",
        "acc_total = []\n",
        "with tf.compat.v1.Session(graph=graph) as sess:\n",
        "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
        "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
        "    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
        "        feed = {input_data: x,\n",
        "                labels_: y,\n",
        "                keep_prob: 1,\n",
        "                initial_state: test_state}\n",
        "        #print(\"hey\")\n",
        "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
        "        temp+=sess.run(predictions,feed_dict=feed).shape[0]\n",
        "        y_test_predictions=sess.run(predictions,feed_dict=feed)\n",
        "        #confusion matrix\n",
        "        y_testing=sess.run(correct_pred,feed_dict=feed)\n",
        "        true_class = tf.compat.v1.argmax( y, 1 )\n",
        "        predicted_class = tf.compat.v1.argmax( y_testing, 1 )\n",
        "        #print(true_class[:5])\n",
        "        #print(predicted_class[:5])\n",
        "        cm_running_total = None\n",
        "        cm_nupmy_array = sess.run(tf.compat.v1.confusion_matrix(true_class, predicted_class, 16), feed_dict=feed )\n",
        "        ##print(sess.run(tf.compat.v1.confusion_matrix(true_class, predicted_class, 16), feed_dict=feed ))\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        backlabel=names_encode.inverse_transform(y_test_predictions)\n",
        "\n",
        "        truebacklabel=names_encode.inverse_transform(y)\n",
        "        for i in range(0,y_test_predictions.shape[0]):\n",
        "          j=0\n",
        "          mx=0\n",
        "          index=0\n",
        "          while j<16:\n",
        "            if y_test_predictions[i][j]>mx:\n",
        "              mx=y_test_predictions[i][j]\n",
        "              index=i\n",
        "              #print(\"Hey\")\n",
        "            j+=1\n",
        "          j=0\n",
        "          while j<16:\n",
        "            if y_test_predictions[i][j]==mx:\n",
        "              y_test_predictions[i][j]=1\n",
        "            else:\n",
        "              y_test_predictions[i][j]=0\n",
        "              #print(\"Hey2\")\n",
        "            j+=1\n",
        "          j=0\n",
        "          i+=15\n",
        "        backlabel=names_encode.inverse_transform(y_test_predictions)\n",
        "        y_test_predictions = y_test_predictions.astype('int32')\n",
        "        \n",
        "        #print(y_test_predictions[:5])\n",
        "        #print(type(y_test_predictions))\n",
        "        #print(y[:5])\n",
        "        #print(backlabel[:5])\n",
        "        #print(truebacklabel[:5])\n",
        "        #y_test_predictions.flatten()\n",
        "        \"\"\"\"if y_test_predictions.any():\n",
        "          y_test_predictions = np.hstack(y_test_predictions)\n",
        "        else:\n",
        "          y_test_predictions = y_test_predictions.flatten()\n",
        "        #y_test_predictionss= np.where(y_test_predictions==max(y_test_predictions),1,0)\n",
        "        print(y_test_predictions)\n",
        "        if y.any():\n",
        "          y = np.hstack(y)\n",
        "        else:\n",
        "          y = y.flatten()\n",
        "        for i in range(0,y_test_predictions.shape[0]):\n",
        "          j=0\n",
        "          mx=0\n",
        "          index=0\n",
        "          while j<16 and i+j<y_test_predictions.shape[0]:\n",
        "            if y_test_predictions[i+j]>mx:\n",
        "              mx=y_test_predictions[i+j]\n",
        "              index=i\n",
        "            j+=1\n",
        "          j=0\n",
        "          while j<16 and i+j<y_test_predictions.shape[0]:\n",
        "            if y_test_predictions[i+j]==mx:\n",
        "              y_test_predictions[i+j]=1\n",
        "            else:\n",
        "              y_test_predictions[i+j]=0\n",
        "            \n",
        "            j+=1\n",
        "          i=i+15\n",
        "        \"\"\"\n",
        "        \n",
        "        for i in range(0,backlabel.shape[0]):\n",
        "          if(truebacklabel[i][0]=='I'):\n",
        "            Label1true.append(1)\n",
        "          else:\n",
        "            Label1true.append(0)\n",
        "          if(truebacklabel[i][1]=='N'):\n",
        "            Label2true.append(1)\n",
        "          else:\n",
        "            Label2true.append(0)\n",
        "          \n",
        "          if(truebacklabel[i][2]=='F'):\n",
        "            Label3true.append(1)\n",
        "          else:\n",
        "            Label3true.append(0)\n",
        "          \n",
        "          if(truebacklabel[i][3]=='J'):\n",
        "            Label4true.append(1)\n",
        "          else:\n",
        "            Label4true.append(0)\n",
        "          \n",
        "          if(backlabel[i][0]=='I'):\n",
        "            Label1pred.append(1)\n",
        "          else:\n",
        "            Label1pred.append(0)\n",
        "\n",
        "          if(backlabel[i][1]=='N'):\n",
        "            Label2pred.append(1)\n",
        "          else:\n",
        "            Label2pred.append(0)\n",
        "          \n",
        "          if(backlabel[i][2]=='F'):\n",
        "            Label3pred.append(1)\n",
        "          else:\n",
        "            Label3pred.append(0)\n",
        "          \n",
        "          if(backlabel[i][3]=='J'):\n",
        "            Label4pred.append(1)\n",
        "          else:\n",
        "            Label4pred.append(0)\n",
        "          \n",
        "          \n",
        "        print(Label4pred[:5])\n",
        "        print(Label4true[:5])\n",
        "        f1=f1_score(y_true=y, y_pred=y_test_predictions, average='weighted')\n",
        "        #set() - set(y_test_predictions)\n",
        "        re=recall_score(y_true=y, y_pred=y_test_predictions, average='weighted')\n",
        "        pr=precision_score(y_true=y, y_pred=y_test_predictions, average='weighted')\n",
        "        test_acc.append(batch_acc)\n",
        "        f1_total.append(f1)\n",
        "        recall_total.append(re)\n",
        "        prec_total.append(pr)\n",
        "        acc_total.append(ac)\n",
        "        f1=f1_score(y_true=y, y_pred=y_test_predictions, average='weighted')\n",
        "        re=recall_score(y_true=y, y_pred=y_test_predictions, average='weighted')\n",
        "        pr=precision_score(y_true=y, y_pred=y_test_predictions, average='weighted')\n",
        "     \n",
        "   # print(test_acc)\n",
        "   # print(y_test_predictions)\n",
        "    #print(\"Class1: \")\n",
        "    f1=f1_score(y_true=Label1true, y_pred=Label1pred)\n",
        "    print(\"Class 1 F1 score: {:.5f}\".format(f1))\n",
        "    re=recall_score(y_true=Label1true, y_pred=Label1pred)\n",
        "    pr=precision_score(y_true=Label1true, y_pred=Label1pred)\n",
        "    ac=accuracy_score(y_true=Label1true, y_pred=Label1pred)\n",
        "    print(\"Recall score: {:.5f}\".format(re))\n",
        "    print(\"Precision score: {:.5f}\".format(pr))\n",
        "    print(\"Accuracy score: {:.5f}\".format(ac))\n",
        "    print(\"Class2: \")\n",
        "    f1=f1_score(y_true=Label2true, y_pred=Label2pred, average='weighted')\n",
        "    re=recall_score(y_true=Label2true, y_pred=Label2pred)\n",
        "    pr=precision_score(y_true=Label2true, y_pred=Label2pred)\n",
        "    ac=accuracy_score(y_true=Label2true, y_pred=Label2pred)\n",
        "    \n",
        "    print(\"F1 score: {:.5f}\".format(f1))\n",
        "    print(\"Recall score: {:.5f}\".format(re))\n",
        "    print(\"Precision score: {:.5f}\".format(pr))\n",
        "    print(\"Accuracy score: {:.5f}\".format(ac))\n",
        "    #f1=f1_score(y_true=Label3true, y_pred=Label3pred, average='weighted')\n",
        "    print(\"Class3: \")\n",
        "    f1=f1_score(y_true=Label3true, y_pred=Label3pred, average='weighted')\n",
        "    re=recall_score(y_true=Label3true, y_pred=Label3pred)\n",
        "    pr=precision_score(y_true=Label3true, y_pred=Label3pred)\n",
        "    ac=accuracy_score(y_true=Label3true, y_pred=Label3pred)\n",
        "    print(\"F1 score: {:.5f}\".format(f1))\n",
        "    print(\"Recall score: {:.5f}\".format(re))\n",
        "    print(\"Precision score: {:.5f}\".format(pr))\n",
        "    print(\"Accuracy score: {:.5f}\".format(ac))\n",
        "    print(\"Class4: \")\n",
        "    f1=f1_score(y_true=Label4true, y_pred=Label4pred, average='weighted')\n",
        "    re=recall_score(y_true=Label4true, y_pred=Label4pred)\n",
        "    pr=precision_score(y_true=Label4true, y_pred=Label4pred)\n",
        "    \n",
        "    print(\"F1 score: {:.5f}\".format(f1))\n",
        "    print(\"Recall score: {:.5f}\".format(re))\n",
        "    print(\"Precision score: {:.5f}\".format(pr))\n",
        "    print(\"Accuracy score: {:.5f}\".format(ac))\n",
        "   \n",
        "    #print(\"F1 score: {:.5f}\".format(np.mean(f1_total)))\n",
        "    print(\"Test accuracy: {:.5f}\".format(np.mean(test_acc)))\n",
        "    #print(\"Recall: {:.5f}\".format(np.mean(recall_total)))\n",
        "    #print(\"Precision: {:.5f}\".format(np.mean(prec_total)))\n",
        "    \n",
        "    #f1=(2*pr*re)/(pr+re)\n",
        "    #print(f1)"
      ],
      "metadata": {
        "id": "ugbHrMCKunNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G98eA2Rf5vXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9MaP4Ka4rNq"
      },
      "outputs": [],
      "source": [
        "y_test_predictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w2WG_l97VcL"
      },
      "outputs": [],
      "source": [
        "temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBI2U9VS9EsL"
      },
      "outputs": [],
      "source": [
        "true_pos\n",
        "for i in range(0,y_test_predictions.shape[0]):\n",
        "  for j in (0,y_test_predictions.shape[1]):\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkM64j1I1Ya8"
      },
      "outputs": [],
      "source": [
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5M0rjPbu25KL"
      },
      "outputs": [],
      "source": [
        "test_y.shape\n",
        "np.concatenate((test_y,y_test_predictions),axis=0)\n",
        "test_y.shape\n",
        "#test_x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dn_GY1-33ev-"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score\n",
        ")\n",
        "\n",
        "# Generate confusion matrix for the predictions\n",
        "conf_matrix = confusion_matrix(test_y, y_test_predictions)\n",
        "conf_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47672Pqtsxxt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}